\setlength{\absparsep}{18pt} % ajusta o espaçamento dos parágrafos do resumo
\begin{resumo}
    Modelos de regressão são procedimentos estatísticos que permitem estimar as relações entre as variáveis que modelam um determinado problema. É comum dividir essas relações em duas categorias: lineares e não-lineares. Quando deseja-se solucionar um problema utilizando o arcabouço de Aprendizado de Máquina (AM), é comum observar uma preferência pelo uso de modelos que estimem relações lineares. As motivações subjacentes encontram-se em formulações matemáticas mais simples que gozam de propriedades bastante conhecidas e, ademais, resultam muitas vezes em soluções analíticas. Contudo, a notória desvantagem no uso de modelos lineares é a falta de poder de generalização para problemas de cunho real; isso ocorre devido à forte natureza não-linear de diversos eventos que ocorrem no dia-a-dia. Para escapar à necessidade de criação de modelos não-lineares (que possuem uma complexidade intrínseca maior que à de modelos lineares), o uso de uma estratégia conhecida como truque do \textit{kernel} tornou-se bastante popular. Sua ideia base consiste em transformar o espaço original do problema em um espaço de Hilbert de alta -- ou infinita -- dimensão, onde os padrões apresentem comportamento linear; sendo possível, então, o uso de métodos lineares como \textit{ridge regression} para descoberta das relações entre as variáveis. A escolha de uma função que realize esta transformação espacial (chamada função de \textit{kernel}) é extremamente dependente do problema em mãos, o que levou vários pesquisadores a utilizarem funções já consolidadas, tais como \textit{rbf} e polinomial (com otimização de seus parâmetros). Porém, essa abordagem resulta em variações no desempenho para diferentes conjuntos de dados. Criar funções de \textit{kernel} que se adaptem a um determinado problema é uma solução mais conveniente, mas que gera uma carga de trabalho grande, uma vez que é necessário conhecer detalhes do problema. Nesta monografia é apresentada uma proposta baseada em Programação Genética (PG), denominada \textit{Genetic Kernels for Regression} (GKR), que propõe um método de criação automática de novas funções de \textit{kernel} adaptáveis para problemas de regressão. O método utiliza operadores de fechamento, que combinam funções de \textit{kernel} existentes para criação de novos \textit{kernels} válidos. Cada função é uma árvore sintática, onde cada nó possui um conjunto de restrições que garante a validade do \textit{kernel} criado. O modelo proposto tem seu desempenho comparado com modelos estado-da-arte, tais como \textit{Support Vector Regression} (SVR) com \textit{kernels} linear, \textit{rbf} e polinomial, e Redes Neurais Artificiais (RNA) dos tipos \textit{Multilayer Perceptron} (MLP) e \textit{Radial Basis Functions} (RBF). Os resultados permitem inferir que: (\textit{i}) o modelo proposto é bastante robusto quando apresentado a diferentes conjuntos de dados; (\textit{ii}) possui desempenho similar ao dos modelos SVR, MLP e RBF, chegando à uma leve superioridade para a maioria dos problemas utilizados nos experimentos.
    
    \textbf{Palavras-chave}: funções de \textit{kernel}, problemas de regressão, programação genética, \textit{ridge regression}, truque do \textit{kernel}.
\end{resumo}